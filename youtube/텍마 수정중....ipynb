{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # 웹 페이지 소스를 얻기 위한 패키지(기본 내장 패키지이다.)\n",
    "from bs4 import BeautifulSoup # 웹 페이지 소스를 얻기 위한 패키지, 더 간단히 얻을 수 있다는 장점이 있다고 한다.\n",
    "from datetime import datetime                                # (!pip install beautifulsoup4 으로 다운받을 수 있다.)\n",
    "import pandas as pd # 데이터를 처리하기 위한 가장 기본적인 패키지\n",
    "import time # 사이트를 불러올 때, 작업 지연시간을 지정해주기 위한 패키지이다. (사이트가 늦게 켜지면 에러가 발생하기 때문)\n",
    "import urllib.request #\n",
    "import json\n",
    "import re     \n",
    "import datetime as dt\n",
    "import os\n",
    "import glob\n",
    "from pandas import DataFrame\n",
    "from collections import Counter\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from wordcloud import WordCloud\n",
    "#import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.font_manager as fm\n",
    "font_name = fm.FontProperties(fname = 'C:\\\\Windows\\\\Fonts\\\\malgun.ttf').get_name()\n",
    "plt.rc('font', family = font_name)\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= pd.read_csv('불용어사전(한글자).csv',encoding='cp949')\n",
    "stop_list = list(f)\n",
    "Unnamed = 'Unnamed: '+ str(len(stop_list)-1)\n",
    "stop_list.extend(['거','저','내','더','용','앞','윤','분','늘','멀','욬','수','번','안','이','제','요','안','뭐','더','전'])\n",
    "stop_words = set(stop_list)\n",
    "\n",
    "stop_words.remove(Unnamed)\n",
    "\n",
    "# stop_words\n",
    "\n",
    "# 단어 새로 추가했을 때에 사용!!!\n",
    "f = open('불용어사전(한글자).csv','w')\n",
    "for i in stop_words:\n",
    "    f.write(i+',')\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유투브 채널 이름 입력하시오.로즈하\n",
      "2020. 5. 23.\n",
      "2020. 5. 7.\n",
      "2020. 4. 26.\n",
      "2020. 4. 15.\n",
      "2020. 4. 8.\n",
      "2020. 4. 7.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acorn\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1bef326737466e8ddb30852b458a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 댓글에 날짜 추가!!!\n",
    "youtube_channel = input(\"유투브 채널 이름 입력하시오.\")\n",
    "date = pd.read_csv('./Beauty/'+youtube_channel+'/'+youtube_channel+'_info.csv')\n",
    "ccc = date[date['start_date'] >= '2020. 4. 1']['start_date']\n",
    "\n",
    "ccc\n",
    "for i in ccc:\n",
    "    print(i)\n",
    "# pd.read_csv('./Beauty/'+youtube_channel+'/comment/'+youtube_channel+'_comment_[i+1].csv')\n",
    "\n",
    "for i in range(len(ccc)):\n",
    "    print(i)\n",
    "#     a = pd.read_csv(('./Beauty/'+youtube_channel+'/comment/'+youtube_channel+'_comment_{}.csv').format(i+1))['youtube'].values\n",
    "#     np.append(comment_date, a)     up_date = pd.read_csv(('./Beauty/'+youtube_channel+'/comment/'+youtube_channel+'_comment_{}.csv').format(i+1))\n",
    "#     comment_date.append(up_date)\n",
    "comment_date = []\n",
    "for i in range(len(ccc)):\n",
    "     print(i)\n",
    "     a = pd.read_csv(('./Beauty/'+youtube_channel+'/comment/'+youtube_channel+'_comment_{}.csv').format(i+1))\n",
    "     comment_date.append(a)\n",
    "        \n",
    "comment_date[0]\n",
    "for i in tqdm_notebook(range(len(ccc))):\n",
    "    \n",
    "    df = comment_date[i]\n",
    "    you_id = df['youtube_id'].values\n",
    "    co_id = df['comment'].values\n",
    "    likes = df['like_num'].values\n",
    "    start_date = ccc[i]\n",
    "    comment_final = pd.DataFrame(data = {'닉네임':you_id,\n",
    "                                '댓글':co_id,\n",
    "                                '좋아요수':likes,\n",
    "                                '날짜':start_date})\n",
    "    comment_final.to_csv('./Beauty/'+youtube_channel+'/comment/'+ youtube_channel +'_comment_date{}.csv'.format(i+1),\n",
    "                        encoding='utf-8-sig',index=False)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6d9386ed1445>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mallData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdataCombine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallData\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdataCombine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m         \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No objects to concatenate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# 파일 합치는 곳 (폴더 제목 입력)\n",
    "# youtube_channel = input(\"유투브 채널 이름 입력하시오.\")\n",
    "path =  str(youtube_channel) + \"/comment\"\n",
    "path2 = str(youtube_channel) + '/result.csv'\n",
    "arg_youtube = str(youtube_channel) + \"*\"\n",
    "\n",
    "\n",
    "allFile_list = glob.glob(os.path.join(path,arg_youtube))\n",
    "print(allFile_list)\n",
    "allData = []\n",
    "for file in allFile_list:\n",
    "    df = pd.read_csv(file)\n",
    "    allData.append(df)\n",
    "    \n",
    "dataCombine = pd.concat(allData,axis =0, ignore_index = True)\n",
    "\n",
    "dataCombine.to_csv(path2, index = False)\n",
    "cdPath = str(youtube_channel) + '/result.csv'\n",
    "comment_data = pd.read_csv(cdPath)\n",
    "comment_data ##제대로 출력되는지 확인\n",
    "com_list = list(comment_data['comment'])   ##제대로 출력되는지 확인\n",
    "# com_list   ##제대로 출력되는지 확인\n",
    "\n",
    "# ### 추출된 단어가 무슨 형태인지 파악\n",
    "# # for sentence in com_list:\n",
    "# #     morph = twitter.pos(sentence)\n",
    "# #     sentences_tag.append(morph)\n",
    "\n",
    "# 추출 단어들 저장\n",
    "\n",
    "def get_noun(comment_txt):\n",
    "    \n",
    "    twitter = Twitter()\n",
    "    noun_list = []\n",
    "    \n",
    "    if len(comment_txt) > 0:\n",
    "        twitter = twitter.pos(comment_txt, norm = True, stem = True)\n",
    "        for word, tag in twitter:\n",
    "            if tag in ['Noun']:\n",
    "                \n",
    "                if word not in stop_words:\n",
    "                    noun_list.append(word)\n",
    "                    \n",
    "    return noun_list\n",
    "\n",
    "def get_adj(comment_txt):\n",
    "\n",
    "    twitter = Twitter()\n",
    "    adj_list = []\n",
    "    \n",
    "    if len(comment_txt) > 0:\n",
    "        twitter = twitter.pos(comment_txt, norm = True, stem = True)\n",
    "        for word, tag in twitter:\n",
    "            if tag in ['Adjective']:\n",
    "                if word not in stop_words:\n",
    "                    adj_list.append(word)\n",
    "                    \n",
    "    return adj_list\n",
    "\n",
    "def get_verb(comment_txt):\n",
    "\n",
    "    twitter = Twitter()\n",
    "    verb_list = []\n",
    "    \n",
    "    if len(comment_txt) > 0:\n",
    "        twitter = twitter.pos(comment_txt, norm = True, stem = True)\n",
    "        for word, tag in twitter:\n",
    "            if tag in ['Verb']:\n",
    "                if word not in stop_words:\n",
    "                    verb_list.append(word)\n",
    "    return verb_list\n",
    "\n",
    "comment_list = []\n",
    "for i in range(len(comment_data)):\n",
    "    comment_list.append(comment_data['comment'].iloc[i])\n",
    "    \n",
    "comment_result = []\n",
    "\n",
    "for i in comment_list:\n",
    "    comment_result.append(i)\n",
    "    \n",
    "comment_result = pd.DataFrame(comment_result, columns=[\"comment\"])\n",
    "    \n",
    "#cloud.to_file(youtube_channel+'/{}wordcloud.png'.format(youtube_channel)) # 해당폴더에 자동저장\n",
    "comment_result['noun'] = comment_result['comment'].apply(lambda x: get_noun(x))\n",
    "comment_result['adj'] = comment_result['comment'].apply(lambda x: get_adj(x))\n",
    "comment_result['verb'] = comment_result['comment'].apply(lambda x: get_verb(x))\n",
    "\n",
    "comment_result.to_csv(youtube_channel +'/{}token_result.csv'.format(youtube_channel),\n",
    "                                                                   index = False, encoding = 'utf-8-sig')\n",
    "\n",
    "\n",
    "twitter = Twitter()\n",
    "\n",
    "sentences_tag = []\n",
    "for sentence in tqdm_notebook(com_list):\n",
    "     morph = twitter.pos(sentence)\n",
    "     sentences_tag.append(morph)\n",
    "\n",
    "stop_words\n",
    "for sentence in comment_data:\n",
    "    morph = twitter.pos(sentence)\n",
    "    sentences_tag.append(morph)\n",
    "#     print(morph)\n",
    "#     print('-'*30)\n",
    "\n",
    "# print(sentences_tag)\n",
    "# print(len(sentences_tag))\n",
    "# print('\\n'*3)\n",
    "\n",
    "noun_adj_list = []\n",
    "for sentence1 in sentences_tag:\n",
    "    for word, tag in sentence1:\n",
    "        if tag in ['Noun','Adjective','verb']:\n",
    "            if word not in stop_words:\n",
    "                noun_adj_list.append(word)            \n",
    "\n",
    "counts = Counter(noun_adj_list)\n",
    "select_data = counts.most_common(50)\n",
    "\n",
    "\n",
    "wc = WordCloud(font_path='NanumGothic.ttf',max_font_size=200,background_color='white' ,width=800, height=600)\n",
    "    \n",
    "print(dict(select_data))\n",
    "cloud = wc.generate_from_frequencies(dict(select_data))\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.axis('off')\n",
    "plt.imshow(cloud,interpolation='bilinear')\n",
    "plt.show()\n",
    "cloud.to_file(youtube_channel+'/{}wordcloud.png'.format(youtube_channel)) # 해당폴더에 자동저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
